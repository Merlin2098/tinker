name: llm_inference_optimization
version: "1.0.0"
category: Planning
tier: core
cluster: meta
priority: 85
purpose: "Token budget, context pruning at 60%, temperature selection"
triggers:
  keywords: [token budget, context optimization, inference quality, temperature selection, context pruning]
  match_extensions: []
  match_phases: []
  match_errors: []
requires: []
exclusive_with: []
meta: true
modes: [ANALYZE_AND_IMPLEMENT]
agent_binding:
  mandatory: [agent_inspector, agent_executor, agent_senior]
  optional: []
body_file: "llm_inference_optimization.md"
token_estimate: 600
execution:
  entrypoint: "agent_tools/wrappers/policy_guidance_wrapper.py::run"
  command: ".venv/Scripts/python.exe agent_tools/run_wrapper.py --skill llm_inference_optimization --args-json '<json-object>'"
  args_schema:
    type: object
    required: []
    properties:
      objective:
        type: string
      target_paths:
        type: array
        items:
          type: string
      constraints:
        type: array
        items:
          type: string
      output_mode:
        type: string
      max_items:
        type: integer
